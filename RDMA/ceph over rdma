设计：
	我们的设计围绕着Ceph三种特性所依赖的共同特征：效率（包括时间效率，空间效率）来展开。论述层次是自下向上的模式，从网络I/O到内存管理
  再到CPU，最后是应用层的线程资源竞争层面。
	首先是网络I/O层，我们使用RDMA的SEND,RECV操作原语来进行网络数据传输，采用这种模式是因为在Ceph上层设定了每次向下给网络模块传输的数据要小于4MB，所
  以数据大小是固定在一个范围内，相应的使用固定大小的WORK REQUEST就能简化数据传送路径，原本的模式是在每次传输前，发送方要告诉接收方数据大小，然后接
  收方在自己的内存区申请一个合适大小的缓冲区来放进RECV queue中，这样需要一次通信后才能继续后面的数据传输，而采用双方约定好固定大小并且预先注册好的
  缓冲区后，每次传输就可以直接发送数据而不必等待对方申请完成才继续。但是这会引出另一个问题，数据切片问题，当上层交付下来的数据比当前缓冲区大时，需要
  手动对数据进行分割然后放进多个缓冲区中，再通过work request 来post到send queue中，不过这是在本地机器上处理完成的，相对于远程通信然后注册合适大小
  的缓冲区来说，时间开销上是很小的，因为注册好的缓冲区可以直接被应用层访问，所以没有数据复制过程，并且数据分割是在应用层向网络层写入数据时动态分割
  的，所以没有显式的一个分割过程，是一种过程覆盖的方式。关于缓冲区大小设定的问题，在后面的内存管理会详细讲解。在这里，我们更关心另一个问题，NIC缓
  存问题。众所周知，NIC缓存中主要存放Queue Pair的相关数据和虚实地址转换表数据。但是NIC缓存大小很有限，当NIC缓存溢出时，网卡会通过DMA来将数据移到
  内存中去存放，此时，NIC和主存的关系就相当于缓存和内存的关系，这样以来，一旦缓存过多，就会出现NIC缓存的数据在内存和NIC之间换入换出。这一问题的来
  源主要是QP数量过多或者虚实地址转换表过大。针对QP数量过多问题，可以采用的有多链接共享一个QP，在连接上手动实现QP复用功能，在Ceph环境下，大型的OSD
  集群与相对小型的MON集群要维持通信，虽然为了避免单点失效和负载过重问题将MON做成集群，但是面对中型和大型集群时，单个MON结点需要维持的连接数仍然很
  多(设OSD集群为N，MON集群为M，平均每个MON需要维持的连接最少为N/M，M推荐的最小数目是3)，此时需要的QP数量也就很多，那么超出NIC缓存上限时就会发生
  换入传出现象。针对虚实地址转换表过大问题，我们给一个简单的模型来说明问题，假设NIC要访问一块64K大小且页对齐的内存，设定当前系统采用的页大小是
  4K，那么该内存就包含了16个系统页，则在NIC的虚实地址转换表上对该块内存区的地址条目就有16项，因为NIC访问内存也是按照页大小来访问的。所以为了减少地
  址条目，我们通过在预先申请注册内存时使用大页(linux下为2M)来申请，这样一来，即便传输数据有4M(见上)，NIC中也只需要存储两个地址条目即可。总体来
  说，通过共享QP和启用大页，可以提高NIC缓存命中率和减少换入换出带来的额外时延和性能下降，在网络层加速整个系统的效率，为上层加速打基础。（测试，没
  有使用共享QP和没有启用大页所表现的性能和启用优化之后带来的性能提升，要详细）
	
	其次是内存管理层，在传统TCP/IP通信方式下，应用发送数据所需要的内存都是由系统来管理和释放，但使用RDMA则需要使用者自己来管理，因为NIC要访问内存
  前，需要将该内存通过注册方式来添加到保护页表中防止换入换出，并且返回访问密钥，只有持有该密钥才能访问当前内存块。虽然RDMA有众多优势，但是内存注
  册这块却为人所诟病。因为内存注册过程冗长且耗时，给整个传输造成了很大的性能损失，为了解决这一问题，前人采用的方法都是事先申请已经注册过的适量的
  内存区，然后直接交给应用层使用，应用层使用完了再归还给网络层但并不注销，直到应用停止再统一把申请的内存归还给系统。众所周知，整个系统的效率依赖
  于各个模块的效率，内存管理是影响系统效率的关键因素。具体分为内存利用率和内存使用频率。（32K，256K，15360K）。申请的内存缓冲区大小是影响内存利
  用率的一个因素，同时也是影响应用层在向网络层进行数据交付过程中时间长短的因素。因为内存过大，而实际存储的数据不多，则导致内存利用率低，占有虚实
  地址转换表的空间并且发送数据时需要将整块缓冲区的内容都取一遍通过NIC发送出去，但是好处是申请一次就可以满足需求而不必申请多次才能存储数据；如果内
  存过小，则每次要申请多个内存块才能放满数据，但好处是内存利用率高，产生很少的内存浪费。为了结合两者的优点，我们测出真实环境中Ceph应用向下传输的
  数据大小在使用64K大小的缓冲区时(是否真实?)内存利用率和申请内存块数达到最优点(?)。此外，我们还做了一些额外的优化工作，我们利用非一致性内存访问
  特性来加速内存访问，在每次给应用层分配内存区时，我们总是优先分配最近使用过并归还的内存区，因此此时有很大几率它仍然驻留在cache中，我们实验所用
  的机器缓存参数是：L1 32K，L2 256K，L3 15360K.（万一内存不够用怎么办，是应该达到某一个程度时提前申请好，然后随着时间的推移逐渐去注销恢复到正
  常水平还是说维持不变）
	测试随机分配内存区和使用NUMA特性来分配时性能的差别

	在CPU使用层面，我们同样是关注效率问题，具体分为CPU负载均衡和核私有cache的命中率。应用层如果不关心应用线程是在哪个核上运行，只是注重应用线程的
  任务执行流程的话，那么操作系统会自动进行负载均衡，但是由于操作系统不关心应用层的逻辑，这样就会出现一些问题。当多个线程共享一个QP时(FaRM)，如
  果让操作系统自动进行负载均衡，那么就会出现两个使用同一个QP的线程在不同的核上运行，访问共享资源时，两个核的缓存上都会出现共享资源，则由于每次更
  新共享资源时数据会在两个核的缓存中弹来弹去，造成效率低下的问题；另外，在操作系统自动负载均衡的情况下，线程间切换会带来频繁的核间数据迁移，因
  此，为了解决以上问题，我们使用单线程多链接进行共享QP，将多个逻辑链接安排到同一个QP上，在数据发送和接收时进行复用包装，也就是说处理不同链接的
  数据和请求时不需要线程的切换，只需要将请求排队然后线程处理请求并把数据分流给各个连接即可，将复用做到数据层而不是线程层；另外我们将这个线程绑
  定到特定的核上来保证线程不会在多个核间迁移，进而减少不必要的开销。

	在多线程共享QP时会出现资源竞争问题，
