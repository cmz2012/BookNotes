设计：
	我们的设计围绕着Ceph三种特性所依赖的共同特征：效率（包括时间效率，空间效率）来展开。论述层次是自下向上的模式，从网络I/O到内存管理再到CPU，
	最后是应用的线程资源竞争层面和数据分流层。
	首先是网络I/O层，我们使用RDMA的SEND,RECV操作原语来进行网络数据传输，采用这种模式是因为在Ceph上层设定了每次向下给网络模块传输的数据要小于
	4MB，所以数据大小是固定在一个范围内，相应的使用固定大小的WORK REQUEST就能简化数据传送路径，原本的模式是在每次传输前，发送方要告诉接收方数
	据大小，然后接收方在自己的内存区申请一个合适大小的缓冲区来放进RECV queue中，这样需要一次通信后才能继续后面的数据传输，而采用双方约定好固定
	大小并且预先注册好的缓冲区后，每次传输就可以直接发送数据而不必等待对方申请完成才继续。但是这会引出另一个问题，数据切片问题，当上层交付下来的
	数据比当前缓冲区大时，需要手动对数据进行分割然后放进多个缓冲区中，再通过work request 来post到send queue中，不过这是在本地机器上处理完
	成的，相对于远程通信然后注册合适大小的缓冲区来说，时间开销上是很小的，因为注册好的缓冲区可以直接被应用层访问，所以没有数据复制过程，并且数
	据分割是在应用层向网络层写入数据时动态分割的，所以没有显式的一个分割过程，是一种过程覆盖的方式。关于缓冲区大小设定的问题，在后面的内存管理
	会详细讲解。在这里，我们更关心另一个问题，NIC缓存问题。众所周知，NIC缓存中主要存放Queue Pair的相关数据和虚实地址转换表数据。但是NIC缓存
	大小很有限，当NIC缓存溢出时，网卡会通过DMA来将数据移到内存中去存放，此时，NIC和主存的关系就相当于缓存和内存的关系，这样以来，一旦缓存过
	多，就会出现NIC缓存的数据在内存和NIC之间换入换出。这一问题的来源主要是QP数量过多或者虚实地址转换表过大。针对QP数量过多问题，可以采用的有
	多链接共享一个QP，在连接层手动实现QP复用功能，并且使用共享接收队列来缓解NIC缓存压力，在Ceph环境下，大型的OSD集群与相对小型的MON集群要维
	持通信，虽然为了避免单点失效和负载过重问题将MON做成集群，但是面对中型和大型集群时，单个MON结点需要维持的连接数仍然很多(设OSD集群为N，MON
	集群为M，平均每个MON需要维持的连接最少为N/M，M推荐的最小数目是3)，此时需要的QP数量也就很多，那么超出NIC缓存上限时就会发生换入传出现象。
	针对虚实地址转换表过大问题，我们给一个简单的模型来说明问题，假设NIC要访问一块64K大小且页对齐的内存，设定当前系统采用的页大小是4K，那么该
	内存就包含了16个系统页，则在NIC的虚实地址转换表上对该块内存区的地址条目就有16项，因为NIC访问内存也是按照页大小来访问的。所以为了减少地址
	条目，我们通过在预先申请注册内存时使用大页(linux下为2M)来申请，这样一来，即便传输数据有4M(见上)，NIC中也只需要存储两个地址条目即可。总
	体来说，通过共享QP和启用大页，可以提高NIC缓存命中率和减少换入换出带来的额外时延和性能下降，在网络层加速整个系统的效率，为上层加速打基础。
	（测试，没有使用共享QP和没有启用大页所表现的性能和启用优化之后带来的性能提升，要详细）
	
	其次是内存管理层，在传统TCP/IP通信方式下，应用发送数据所需要的内存都是由系统来管理和释放，但使用RDMA则需要使用者自己来管理，因为NIC要访
	问内存前，需要将该内存通过注册方式来添加到保护页表中防止换入换出，并且返回访问密钥，只有持有该密钥才能访问当前内存块。虽然RDMA有众多优
	势，但是内存注册这块却为人所诟病。因为内存注册过程冗长且耗时，给整个传输造成了很大的性能损失，为了解决这一问题，前人采用的方法都是事先申
	请已经注册过的适量的内存区，然后直接交给应用层使用，应用层使用完了再归还给网络层但并不注销，直到应用停止再统一把申请的内存归还给系统。众
	所周知，整个系统的效率依赖于各个模块的效率，内存管理是影响系统效率的关键因素。具体分为内存利用率和内存使用频率。（32K，256K，15360K）。
	申请的内存缓冲区大小是影响内存利用率的一个因素，同时也是影响应用层在向网络层进行数据交付过程中时间长短的因素。因为内存过大，而实际存储的
	数据不多，则导致内存利用率低，占有虚实地址转换表的空间并且发送数据时需要将整块缓冲区的内容都取一遍通过NIC发送出去，但是好处是申请一次就
	可以满足需求而不必申请多次才能存储数据；如果内存过小，则每次要申请多个内存块才能放满数据，但好处是内存利用率高，产生很少的内存浪费。为了
	结合两者的优点，我们测出真实环境中Ceph应用向下传输的数据大小在使用64K大小的缓冲区时(是否真实?)内存利用率和申请内存块数达到最优点(?)。此
	外，我们还做了一些额外的优化工作，我们利用非一致性内存访问特性来加速内存访问，在每次给应用层分配内存区时，我们总是优先分配最近使用过并归
	还的内存区，因此此时有很大几率它仍然驻留在cache中，我们实验所用的机器缓存参数是：L1 32K，L2 256K，L3 15360K.（万一内存不够用怎么办，是
	应该达到某一个程度时提前申请好，然后随着时间的推移逐渐去注销恢复到正常水平还是说维持不变）
	测试随机分配内存区和使用NUMA特性来分配时性能的差别

	在CPU使用层面，我们同样是关注效率问题，具体分为CPU负载均衡和核私有cache的命中率。应用层如果不关心应用线程是在哪个核上运行，只是注重应用
	线程的任务执行流程的话，那么操作系统会自动进行负载均衡，但是由于操作系统不关心应用层的逻辑，这样就会出现一些问题。当多个线程共享一个QP
	时(FaRM)，如果让操作系统自动进行负载均衡，那么就会出现两个使用同一个QP的线程在不同的核上运行，访问共享资源时，两个核的缓存上都会出现共享
	资源，则由于每次更新共享资源时数据会在两个核的缓存中弹来弹去，造成效率低下的问题；另外，在操作系统自动负载均衡的情况下，线程间切换会带来
	频繁的核间数据迁移，因此，为了解决以上问题，我们使用多线程多链接进行共享QP，将多个逻辑链接安排到同一个QP上，在数据发送和接收时进行复用包
	装，也就是说处理不同链接的数据和请求时不需要线程的切换，只需要将请求排队然后线程处理请求并把数据分流给各个连接即可，将复用做到数据层而不
	是线程层；另外我们将这个线程绑定到特定的核上来保证线程不会在多个核间迁移，进而减少不必要的开销。

	在多线程共享QP时会出现资源竞争问题，多个链接在同时向QP递交发送请求时需要一个统一控制，如果对这个资源进行加锁，那么多个线程间必然要先竞争
	获取锁的持有权，然后在执行数据递交，最后释放锁再由剩余的线程进行竞争。有些为了简化竞争，减少阻塞，使用二阶段来获取锁机制，先测试锁，然后
	再创建锁，但是使用锁和信号量会带来额外的延迟，考虑到效率因素，我们不采用加锁机制。取而代之的是，我们使用一个无锁循环队列作为发送缓冲区，适
	用于当下多生产者-单消费者环境，持有链接的线程通过原子操作从一端向里面添加自己的work request，另外有一个专门的线程从这个队列的另一端取
	work request然后放入共享的queue pair中，这样避免了多个线程直接访问QP而造成的资源竞争，减少了锁等待时间。由于一个线程会处理多个链接，这
	对应着多个QP，且所有的QP都关联着相同的接收队列，那么在网络层收到数据后，需要对数据进行分发到相应的QP中，然后每个QP要再分发到关联的缓冲
	区中，专用线程从缓冲区中取出然后解复用再分发到相关联的链接上，这是整个数据接收时的I/O复用框架。在实现可扩展性的同时尽量减少中间耗费，提
	高系统效率。
