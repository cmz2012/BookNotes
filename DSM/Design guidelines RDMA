强调 底层细节比如 独立的PCIe事务和NIC结构
在RDMA能力和应用程序之间找到高效匹配


PCI Express
PCIe是一个层次化协议，每一层的头部会增加中间耗费，这对理解效率很重要。
RDMA操作会产生3种类型的PCIe事务层报文：读请求，写请求和读完成，没有对一次写的事务层反应。

MMIO writes VS DMA reads：
两种从CPU到PCIe设备传输数据的方法
CPU写到映射设备内存来初始化PCIe写。为了避免每个存储指令都产生一个PCIe写，CPU使用了一个叫做 写结合 的优化方法，它把存储的数据
合并来生成cache行大小的PCIe事务。
带有DMA引擎的PCIe设备可以使用DMA从DRAM中读取数据。DMA读不限制于缓存行大小，但是比CPU的读完成合并大小(Crc)大得多的一个读的response被
切分成多个完成。Crc是128字节在我们使用的intel cpu设备上。一次DMA读总是比相同大小的MMIO使用更少的host-to-device PCIe带宽。


PCIe计数器：
  我们的贡献主要依赖于理解在NIC和CPU之间的PCIe交互。我们的分析主要使用用于DMA读的计数和DMA写的计数。
  
RDMAverbs和传输类型
  一旦完成一个verb，请求者(发起一个verb的一方)的NIC选择性地通过把一个CQE DMA到和这个QP关联的CQ中来发出完成信号。
  这两种类型的verbs是内存verbs和消息verbs。
  内存verbs包括RDMA读，写和原子操作。
  消息verbs包括send和recv。这两个操作会牵涉到responder的CPU：send的负载会被responder的cpu post的recv里指定的地址上。
  
RDMA WQEs：
  为了初始化RDMA操作，在requster一边的用户模式NIC网卡在host memory创建WQE，通常，WQE是在预分配的连续内存区域里创建的，每个WQE
  是独立缓存对齐的。
  首先，假设NIC是基于PCIe的设备，就是说NIC并不集成在芯片上。其次，假设NIC内部使用多个处理单元来完成并行化。
  为了讨论使用下面的优化对CPU和PCIe的影响，我们认为从CPU到NIC传输N个D字节大小的WQEs
  减少CPU发起的MMIO:
    如果减少MMIO的数量或者替换成节省CPU和带宽的DMA，CPU效率和RDMA吞吐量都会得到改善。
    CPU通过MMIO给NIC发送消息来初始化网卡操作。
    这个消息可以包含：1.新的WQE 或者 2.对新的WQE的引用
    第一种情况下，WQE通过64字节 写合并的 MMIOs来传输。(WQE-by-MMIOs)(BlueFrame)(PIO)
    第二种情况下，NIC通过一次或多次DMA来读WQE。(Doorbell)(Doorbell)(SDMA)
    WQE-by-MMIOs方法是为低延迟做优化，通常是默认的方法。
    两种优化可以通过减少MMIOs来改善性能：
      1.Doorbell batching：应用一次发出多个WQEs到一个QP，它可以使用一次Doorbell MMIO来做一次batch
      2.WQE shrinking：减少被一个WQE使用的缓存行的数量可以大大地改善吞吐量。
      
  减少NIC发起的DMAs：
    减少DMA数量可以节省NIC的处理能力和PCIe的带宽，改善RDMA的吞吐量。
    注意到上面的batching优化增加了一次DMA读，但是避免了多次MMIOs，这通常是一个好的折中。
    已知的用来减少NIC发起的DMAs包括unsignaled verbs来减少完成的DMA写（CQE）和负载内联来避免负载DMA读（inline data）。
    batching和shrinking也可以用来优化DMA次数。
    NIC必须对每一个完成的RECV DMA一个CQE。和其他种类那种只提醒完成并且是不重要的的verbs不一样，RECV的CQEs包含重要的元数据，
    比如收到数据的大小。NIC通常会为负载和完成产生两个分离的DMA，分别把他们写到应用程序享有的内存和驱动享有的内存里。
    我们假设对应一个RECV的SEND携带X字节的负载。
    Inline RECV：
      如果X很小，NIC把负载封装进CQE里，这个CQE后来会被驱动复制到应用程序指定的地址空间里去。
    Header-only RECV：
      如果X=0，就是没有负载的情况下，在接收方就不会产生负载的DMA。报文的头部里的一些信息被包含在DMA发出的CQE中，可以用来实现应用协议。
      那么就只会产生一次DMA而不是两次DMA
